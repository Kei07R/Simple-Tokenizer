# Simple Tokenizer

## Introduction

This project is a simple implementation of a text tokenizer. A tokenizer is a program that breaks down a stream of text into smaller chunks, called tokens. These tokens can be words, numbers, or punctuation marks. Tokenization is a fundamental step in many Natural Language Processing (NLP) tasks, such as text classification, machine translation, and sentiment analysis.

The goal of this project is to create a tokenizer that can:

- Take a string of text as input.
- Convert the text into a sequence of tokens.
- Reconstruct the original text from the sequence of tokens.

## Workflow

The tokenizer has two main components: an **Encoder** and a **Decoder**.

### Encoder

The encoder takes a string of text and converts it into a list of tokens. Here's a more detailed breakdown of the process:

1.  **Receive User Input**: The encoder starts with a raw text string provided by the user.
2.  **Word Segmentation**: The input string is split into individual words based on spaces.
3.  **Vocabulary Creation**: A vocabulary (or a "map") is maintained to store unique words. For each word:
    - If the word is not already in the vocabulary, it is added, and a new unique integer ID is assigned to it.
4.  **Token Generation**: Each word in the input text is replaced by its corresponding integer ID from the vocabulary.
5.  **Preserving Order & Token Format**: To ensure that the original order of the words can be reconstructed, the index of each word and its token ID are padded with leading zeros to a fixed length (e.g., 4 digits each) and then concatenated into a single string token. For example, a word at index `1` with a token ID of `2` would become `"00010002"`.

**Example:**

If the input is "Hello this is good Hello", the process would be:

- Words: `["Hello", "this", "is", "good", "Hello"]`
- Vocabulary: `{"Hello": 0, "this": 1, "is": 2, "good": 3}`
- Tokens: `["00000000", "00010001", "00020002", "00030003", "00040000"]`

### Decoder

The decoder takes a list of tokens and converts it back into the original text string.

1.  **Receive Token Input**: The decoder receives the list of string tokens generated by the encoder.
2.  **Sort Tokens**: The tokens are sorted lexicographically. Because the word index is at the beginning of the string and is zero-padded, this correctly restores the original order of the words.
3.  **Word-from-Token Lookup**: The token ID is extracted from the latter part of each string token. This ID is then looked up in a reverse vocabulary (mapping IDs back to words) to retrieve the original word.
4.  **Reconstruct String**: The retrieved words are joined together with spaces to form the original string.

**Example:**

Using the tokens from the previous example `["00000000", "00010001", "00020002", "00030003", "00040000"]`:

- Sorted Tokens: `["00000000", "00010001", "00020002", "00030003", "00040000"]` (already sorted)
- Word Lookup: `["Hello", "this", "is", "good", "Hello"]`
- Reconstructed String: `"Hello this is good Hello"`

## How to Run

To run this project locally, simply open the `index.html` file in your web browser.

## Future Extensions

In the future, this tokenizer could be extended to handle more complex scenarios, such as:

- Supporting different languages and their specific tokenization rules.
- Handling of out-of-vocabulary words during the decoding process.
- Incorporating subword tokenization methods for more efficient vocabulary usage.
